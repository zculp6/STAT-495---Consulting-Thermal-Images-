)
}
# Select 20 combinations (you can shuffle or hand-pick these)
combo_indices <- expand.grid(mean_idx = seq_along(mean_pairs), sd_idx = seq_along(sd_pairs))
# Storage for results
results <- vector("list", nrow(combo_indices))
# Loop through selected combos
for (i in seq_len(nrow(combo_indices))) {
m_pair <- mean_pairs[[combo_indices$mean_idx[i]]]
sd_pair <- sd_pairs[[combo_indices$sd_idx[i]]]
results[[i]] <- cutoff_sim(
dist = "Bimodal",
n = 30,
sims = 1000,
target_q01 = 38,
param_1 = list(m_pair),
param_2 = list(sd_pair)
)
}
# Convert nested results into a unified summary data frame
combined_results <- do.call(rbind, lapply(results, function(df) {
data.frame(
percent_above    = df$percent_above[1],
param_1          = paste0("(", df$param_1[1], ", ", df$param_1[2], ")"),
param_2          = paste0("(", df$param_2[1], ", ", df$param_2[2], ")"),
tolerance_above  = df$tolerance_above[1],
stringsAsFactors = FALSE
)
}))
as.data.frame(
combined_results %>%
group_by(param_1) %>%
arrange(desc(tolerance_above), .by_group = TRUE)
)
cutoff_sim <- function(dist, n, mean = 70, param_1 = NULL, param_2 = NULL, target_q01, sims = 1000) {
library(ggplot2)
# Initialize results with columns for derived parameters
results <- data.frame(param_val = numeric(0), percent_above = numeric(0),
param_1 = numeric(0), param_2 = numeric(0),
shape1 = numeric(0), shape2 = numeric(0),
shape = numeric(0), scale = numeric(0))
# Determine which parameter to vary
if (!is.null(param_1)) {
param_list <- param_1
param_type <- "param_1"
} else if (!is.null(param_2)) {
param_list <- param_2
param_type <- "param_2"
} else {
stop("Either param_1 or param_2 must be provided.")
}
for (val in param_list) {
# Set p1, p2 according to which param is varied
p1 <- if (param_type == "param_1") val else param_1
p2 <- if (param_type == "param_2") val else param_2
# Initialize derived params
shape1 <- NA_real_
shape2 <- NA_real_
shape <- NA_real_
scale <- NA_real_
# Compute derived params once per val (outside sims)
if (dist == "None"){
if (is.null(param_1)){
min <- 1
max <- p2
} else if (is.null(param_2)){
min <- p1
max <- 140
} else{
min <- p1
max <- p2
}
}
if (dist == "Beta") {
if (is.null(param_1)) {
shape2 <- p2
shape1 <- mean * shape2 / (1 - mean)
} else {
shape1 <- p1
shape2 <- shape1 / mean - shape1
}
} else if (dist == "Gamma") {
if (is.null(param_1)) {
shape <- mean / p2
scale <- p2
} else {
shape <- p1
scale <- mean / shape
}
} else if (dist == "Weibull") {
shape <- p1
scale <- mean / gamma(1 + 1/shape)
} else if (dist == "Pareto") {
scale <- p1
if (mean <= scale) {
shape <- NA_real_  # invalid mean
} else {
shape <- mean / (mean - scale)
}
}
# Run simulations
q_vals <- replicate(sims, {
temp_dist <- switch(dist,
"None" = {
x <- sample(min:max, n, replace = TRUE)
x - mean(x) + 70
},
"Normal" = rnorm(n, mean = mean, sd = p1),
"Weibull" = rweibull(n, shape = shape, scale = scale),
"Beta" = rbeta(n, shape1 = shape1, shape2 = shape2),
"Gamma" = rgamma(n, shape = shape, scale = scale),
"Lognormal" = rlnorm(n, meanlog = log(mean), sdlog = p1),
"Pareto" = {
if (is.na(shape)) return(rep(NA_real_, n))
VGAM::rpareto(n, scale = scale, shape = shape)
},
"TruncNormal" = {
rtruncnorm(n, a = 0, b = Inf, mean = mean, sd = p1)
},
"Bimodal" = {
# Assume param_1 = list of means, param_2 = list of sds, or keep fixed
m1 <- param_1[[1]][1]
m2 <- param_1[[1]][2]
s1 <- param_2[[1]][1]
s2 <- param_2[[1]][2]
x1 <- rnorm(n, mean = m1, sd = s1)
x2 <- rnorm(n, mean = m2, sd = s2)
x <- ifelse(runif(n) < 0.5, x1, x2)
},
stop("Unsupported distribution")
)
quantile(temp_dist, 0.01)
})
tol_vals <- replicate(sims, {
temp_dist <- switch(dist,
"None" = {
x <- sample(min:max, n, replace = TRUE)
x - mean(x) + 70
nptol.int(x, alpha = 0.05, P = 0.99, side = 1)$'1-sided.lower'
},
"Normal" = {
x <- rnorm(n, mean = mean, sd = p1)
normtol.int(x, alpha = 0.05, P = 0.99, side = 1)$'1-sided.lower'
},
"Weibull" = {
x <- rweibull(n, shape = shape, scale = scale)
exttol.int(x, alpha = 0.05, P = 0.99, side = 1, dist = "Weibull")$'1-sided.lower'
},
"Beta" = {
x <- rbeta(n, shape1 = shape1, shape2 = shape2)
npbetol.int(x, Beta = 0.95, side = 1)$'1-sided.lower'
},
"Gamma" = {
x <- rgamma(n, shape = shape, scale = scale)
gamtol.int(x, alpha = 0.05, P = 0.99, side = 1)$'1-sided.lower'
},
"Lognormal" = {
x <- rlnorm(n, meanlog = log(mean), sdlog = p1)
normtol.int(x, alpha = 0.05, P = 0.99, side = 1, log.norm = TRUE)$'1-sided.lower'
},
"Pareto" = {
if (is.na(shape)) return(rep(NA_real_, n))
x <- VGAM::rpareto(n, scale = scale, shape = shape)
paretotol.int(x, alpha = 0.05, P = 0.99, side = 1)$'1-sided.lower'
},
"TruncNormal" = {
x <- rtruncnorm(n, a = 0, b = Inf, mean = mean, sd = p1)
nptol.int(x, alpha = 0.05, P = 0.99, side = 1)$'1-sided.lower'
},
"Bimodal" = {
# Assume param_1 = list of means, param_2 = list of sds, or keep fixed
m1 <- param_1[[1]][1]
m2 <- param_1[[1]][2]
s1 <- param_2[[1]][1]
s2 <- param_2[[1]][2]
x1 <- rnorm(n, mean = m1, sd = s1)
x2 <- rnorm(n, mean = m2, sd = s2)
x <- ifelse(runif(n) < 0.5, x1, x2)
nptol.int(x, alpha = 0.05, P = 0.99, side = 1)$'1-sided.lower'
},
stop("Unsupported distribution")
)
})
pct_above <- mean(q_vals > target_q01, na.rm = TRUE) * 100
tol_above <- mean(tol_vals > target_q01, na.rm = TRUE) * 100
# Replace NULL param_1 or param_2 with derived params
# Use current value being iterated over
if (dist != "Bimodal"){
param_1_val <- if (param_type == "param_1") val else param_1
param_2_val <- if (param_type == "param_2") val else param_2
} else{
param_1_val <- param_1
param_2_val <- param_2
}
if (is.null(param_1)) {
if (!is.na(shape1)) {
param_1_val <- shape1
} else if (!is.na(shape)) {
param_1_val <- shape
} else {
param_1_val <- NA_real_
}
}
if (is.null(param_2)) {
if (!is.na(shape2)) {
param_2_val <- shape2
} else if (!is.na(scale)) {
param_2_val <- scale
} else {
param_2_val <- NA_real_
}
}
# Add results row
if (dist == "Bimodal"){
results <- rbind(results, data.frame(
param_val = val,
tolerance_above = tol_above,
percent_above = pct_above,
param_1 = param_1_val[[1]],
param_2 = param_2_val[[1]]
))
} else {
results <- rbind(results, data.frame(
param_val = val,
tolerance_above = tol_above,
percent_above = pct_above,
param_1 = param_1_val,
param_2 = param_2_val
))
}
}
# Plot
if (dist != "Bimodal"){
p <- ggplot(results, aes(x = param_val, y = tolerance_above)) +
geom_line(color = "blue", size = 1) +
geom_point(color = "red", size = 2) +
geom_hline(yintercept = 99, linetype = "dashed", color = "black", size = 1) +
labs(
title = paste0(dist, ": Lower Bound > ", target_q01, " across ", param_type),
x = param_type,
y = "% of Tolerance Lower Bound > 38"
) +
theme_minimal()
results <- results %>%
select(percent_above, param_1, param_2, tolerance_above)
return(list(results = results, plot = p))
} else {
results <- results %>%
select(percent_above, param_1, param_2, tolerance_above)
return(results)
}
}
# Storage for results
results <- vector("list", nrow(combo_indices))
# Loop through selected combos
for (i in seq_len(nrow(combo_indices))) {
m_pair <- mean_pairs[[combo_indices$mean_idx[i]]]
sd_pair <- sd_pairs[[combo_indices$sd_idx[i]]]
results[[i]] <- cutoff_sim(
dist = "Bimodal",
n = 30,
sims = 1000,
target_q01 = 38,
param_1 = list(m_pair),
param_2 = list(sd_pair)
)
}
# Convert nested results into a unified summary data frame
combined_results <- do.call(rbind, lapply(results, function(df) {
data.frame(
percent_above    = df$percent_above[1],
param_1          = paste0("(", df$param_1[1], ", ", df$param_1[2], ")"),
param_2          = paste0("(", df$param_2[1], ", ", df$param_2[2], ")"),
tolerance_above  = df$tolerance_above[1],
stringsAsFactors = FALSE
)
}))
as.data.frame(
combined_results %>%
group_by(param_1) %>%
arrange(desc(tolerance_above), .by_group = TRUE)
)
nycflights13::planes
view(planes)
planes <- nycflights13::planes
view(planes)
write.csv(planes, "planes.csv", row.names = FALSE)
as.data.frame(
combined_results %>%
group_by(param_1) %>%
arrange(desc(tolerance_above), .by_group = TRUE)
)
# None
# The min and max are the values - mean of sample dist + 70
max_values <- seq(25,75, by = 1)
no_dist <- cutoff_sim(dist="None", n = 30, param_1 = NULL, param_2 = max_values, target_q01 = 38, sims = 1000)
no_dist
norm_dist
truncnorm
as.data.frame(
combined_results %>%
group_by(param_1) %>%
arrange(desc(tolerance_above), .by_group = TRUE)
)
exp(-1)
exp(0.6)
exp(0.01)
179/0.9
?npbetatol
?npbetatol()
library(tolerance)
?npbetatol()
?npbetol.int()
library(nycflights13)
flights
install.packages("robustbase")
library(robustbase)
aircraft
pilot
aircraft
dat <- aircraft
mod <- lm(y ~ X1 + X2 + X3 + X4, dat)
mod <- lm(Y ~ X1 + X2 + X3 + X4, dat)
summary(mod)
plot(mod,1)
plot(mod,2)
library(lmtest)
bptest(mod_1)
bptest(mod)
ks.test(rstandard(mod),"pnorm")
plot(mod,1)
dat
library(nycflights13)
planes
flights
planes
dat
plot(mod,1)
plot(mod,2)
mod <- lm(Y ~ .*., dat)
summary(mod)
summary(mod)
plot(mod,1)
plot(mod,2)
plot(mod,1)
mod <- lm(Y ~ X1 + X2 + X3 + X4 + X1:X3 + X2:X3 + X2:X4, dat)
summary(mod)
plot(mod,1)
plot(mod,2)
mod <- lm(ln(Y) ~ X1 + X2 + X3 + X4 + X1:X3 + X2:X3 + X2:X4, dat)
summary(mod)
mod <- lm(ln(Y) ~ X1 + X2 + X3 + X4 + X1:X3 + X2:X3 + X2:X4, dat)
mod <- lm(log(Y) ~ X1 + X2 + X3 + X4 + X1:X3 + X2:X3 + X2:X4, dat)
summary(mod)
plot(mod,1)
plot(mod,2)
mod <- lm(Y ~ X1 + X2 + X3 + X4 + X1:X3 + X2:X3 + X2:X4, dat)
summary(mod)
plot(mod,1)
mod <- lm(Y ~ X1 + X2 + X3 + X4 + X1:X3 + X2:X3 + X2:X4 + I(X1^2) + I(X2)^2, dat)
summary(mod)
mod <- lm(Y ~ X1 + X2 + X3 + X4 + X1:X3 + X2:X3 + X2:X4 + I(X1^2) + I(X2^2) + I(X3^2) + I(X4^2), dat)
summary(mod)
step(mod)
new_mod <- lm(Y ~ X1 + X2 + X3 + X4 + I(X1^2) + X1:X3 + X2:X3 + X2:X4, data = dat)
plot(new_mod,1)
plot(new_mod,2)
library(lmtest)
bptest(new_mod)
ks.test(rstandard(new_mod),"pnorm")
plot(new_mod,1)
summary(new_mod)
new_mod <- lm(Y ~ X1 + X2 + X3 + X4 + X1:X3 + X2:X3 + X2:X4, data = dat)
summary(new_mod)
plot(new_mod,1)
plot(new_mod,2)
summary(new_mod)
plot(new_mod,1)
new_mod <- lm(log(Y)~ X1 + X2 + X3 + X4 + X1:X3 + X2:X3 + X2:X4, data = dat)
summary(new_mod)
plot(new_mod,1)
plot(new_mod,2)
new_mod <- lm(log(Y)~ log(X1 + X2 + X3 + X4 + X1:X3 + X2:X3 + X2:X4), data = dat)
summary(new_mod)
new_mod <- lm(log(Y)~ log(X1 + X2 + X3 + X4 + X1:X3 + X2:X3 + X2:X4), data = dat)
new_mod <- lm(log(Y)~ log(X1) + X2 + X3 + X4 + X1:X3 + X2:X3 + X2:X4, data = dat)
summary(new_mod)
new_mod <- lm(sqrt(Y) ~ X1 + X2 + X3 + X4 + X1:X3 + X2:X3 + X2:X4, data = dat)
summary(new_mod)
plot(new_mod,1)
new_mod <- lm(Y ~ X1 + X2 + X3 + X4 + X1:X3 + X2:X3 + X2:X4, data = dat)
summary(new_mod)
plot(new_mod,1)
plot(new_mod,1)
plot(new_mod,2)
anova(dat)
library(robustbase)
dat <- aircraft
anova(dat)
write.csv(dat, "aircraft.csv", row.names = FALSE)
?anova()
mod <- lm(Y ~ .*., data=dat)
anova(mod)
?aircraft
version
exp(-10.65+0.0055*2000)/(1 + exp(-10.65+0.0055*2000))
log(-5.15)
exp(-5.15)
exp(-5.15)/(1+exp(-5.15))
2*51
16*9
16*9
22/144
1/12
144/11
144/2
?log1p
theta <- seq(0,100,by=0.001)
y <- c(43,44,45,46.5,47.5)
loglik <- sapply(theta, function(w) {
sum(1/(1+(y-w)^2))
})
loglik
theta <- seq(0,100,by=0.001)
y <- c(43,44,45,46.5,47.5)
loglik <- sapply(theta, function(w) {
-sum(log1p((y - w)^2))   # log1p(x) = log(1+x), numerically stable
})
# Shift for numerical stability
loglik_shift <- loglik - max(loglik)
# Unnormalized posterior
unnormalized <- exp(loglik_shift)
# Normalize
step <- 0.001
posterior <- unnormalized / (sum(unnormalized) * step)
# Posterior summaries
post_mean <- sum(theta * posterior) * step
cdf <- cumsum(posterior) * step
post_median <- theta[which.min(abs(cdf - 0.5))]
post_map <- theta[which.max(posterior)]
# Plot posterior
plot(theta, posterior, type = "l", lwd = 2,
xlab = expression(theta), ylab = "Posterior density p(theta | y)",
main = "Posterior for theta (Cauchy location, scale=1)")
abline(v = c(post_mean, post_median, post_map), col = c("blue","red","darkgreen"), lty = 2)
legend("topright", legend = c("Mean","Median","MAP"),
col = c("blue","red","darkgreen"), lty = 2, bty = "n")
# Plot posterior
plot(theta, posterior, type = "l", lwd = 2,
xlab = expression(theta), ylab = "Posterior density p(theta | y)",
main = "Posterior for theta (Cauchy location, scale=1)")
posterior
sample <- sample(posterior, n)
n <- 1000
sample <- sample(posterior, n)
hist(sample)
?hist
hist(sample, breaks = 50)
hist(sample, breaks = 100)
hist(sample, breaks = 10)
hist(sample, breaks = 25)
hist(sample, breaks = 100)
hist(sample, breaks = 500)
hist(sample, breaks = 50)
y6 <- rcauchy(1000, location = posterior_samples, scale = 1)
y6 <- rcauchy(1000, location = posterior, scale = 1)
hist(y6, breaks = 50)
hist(y6, breaks = 10)
hist(y6, breaks = 50)
hist(y6, breaks = 150)
min(y6)
min(posterior)
hist(y6, breaks = 150)
install.packages(c("jpeg", "EBImage"))
if (!requireNamespace("BiocManager", quietly = TRUE))
install.packages("BiocManager")
BiocManager::install("EBImage")
setwd("C:/Users/ASUS/OneDrive/Desktop/STAT 495 - Consulting")
img <- readJPEG("CH01.jpeg")
library(jpeg)
img <- readJPEG("CH01.jpeg")
img
dim(img)
img <- readImage("CHO01.jpeg")
library(EBImage)
img <- readImage("CHO01.jpeg")
img <- readImage("CH01.jpeg")
display(img)
gray <- channel(img, "gray")
mask <- gray > 0.9
display(mask)
mask <- gray > 0.8
display(mask)
mask <- gray > 0.6
display(mask)
mask <- gray > 0.65
display(mask)
mask <- gray > 0.7
display(mask)
mask <- gray > 0.6
display(mask)
install.packages("OpenImageR")
library(OpenImageR)
# Apply median filter to masked areas
img_matrix <- imageData(img)
for (ch in 1:3) {
channel_img <- img_matrix[,,ch]
channel_img[mask] <- median(channel_img[!mask])  # Replace with median of surroundings
img_matrix[,,ch] <- channel_img
}
clean_img <- Image(img_matrix, colormode = Color)
display(clean_img)
